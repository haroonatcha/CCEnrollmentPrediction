---
title: 'Predicting Enrollment with Stacked Models: Bringing Together Theory and Empirics'
author: "Haroon Atcha"
date: "6/4/2021"
output:
  html_document: default
  word_document: default
bibliography: Citations.bib
---

# Introduction

Enrollment forecasting is one of the most consequential tasks a given Institutional Research department will undertake. The pressures of getting it 'right' are especially pronounced in the community college setting. As open enrollment institutions that are uniquely reliant on public funding, accurate enrollment forecasts are critical for guaranteeing adequate state-level funding and proper planning. Moreover, community colleges operate under unique data availability constraints and admission processes that make the task difficult.

Despite the importance of this task, the extant literature on enrollment forecasting - especially in the community college context - is slim and relatively disconnected from theoretical advances in the study of the predictors of retention and enrollment. This manifests itself in two main problems. 

First, practitioners operating in open enrollment contexts have little recourse to literature that speaks to their specific circumstances. Much of the literature has been generated by practitioners in selective-enrollment institutions and speaks to issues particular to that context. [@aksenova_enrollment_2006], [@chen_integrated_2008], [@nandeshwar_enrollment_2009], and [@slim_predicting_2018] for instance, exclusively apply their models to the selective enrollment context. Indeed, X's models are built to specifically predict the likelihood of enrollment *after* acceptance; a process that has no direct analogue in the open enrollment context. Work addressing the problem in the community college context is largely dated. While a highly useful survey of forecasting method used at community colleges, [@bender_community_1981]'s work is now several decades old and understandably is not reflective of modern forecasting tools and approaches. Likewise, [@lawrence_forecasting_1980] is useful as a snapshot of best forecasting practices of the time period, but is similarly dated. [@pennington_community_2002] represents the most recent and comprehensive project addressing this issue. Numerous dissertations (see: x, y, and z) have reproduced and expanded this work in varying local contexts, adding important contextual knowledge but few explicitly addressing modeling choices. In all, the literature that speaks to predictive modeling of enrollment in the community college context is limited.

Second, predictive models of enrollment do not reflect our theoretical understanding of the processes that are being modeled. Much of the modeling in the extant literature falls into one of two camps: time series approaches at the aggregate level (ARIMA) or linear modeling techniques (OLS, SVG, etc.). While such approaches produce perfectly acceptable predictions, such models are rarely reflective of the theoretical processes they attempt to capture. [@chen_integrated_2008], for instance, compares predictions from both, ARIMA and linear models though uses the aggregate number of enrollments in a given semester as the target variable despite noting that '[...] the portion of a universityâ€™s student enrollees (freshmen) depends on the number of high school graduates within the state [...] [and] the same pool of eligible enrolled or returning students (one-year lagged OSU enrollment).' The recognition that two distinct sub-populations constitute the population of enrollees is not reflected in the modeling which treats each semester's cohort uniformly^[See also [@pfitzner_shortterm_1987] for an early implementation of time series modeling in the community college context that approaches the problem at the aggregate level]. By contrast, [@trusheim_predictive_2011] does model returning and new enrollees separately, and while this serves as a good example of using theoretical knowledge as a foundation for in empirical exercises, the methods used to achieve predictions are fairly unsophisticated.

Here, I argue that, in the interest of predictive accuracy and knowledge accumulation, our statistical models ought to be theoretically informed. To demonstrate this, I present a hypothetical example with generated data, an approach widely used in the study of statistics. I show that bringing theoretical knowledge to bear on predictive problems - even if that knowledge is tentative or incomplete - is a fruitful avenue for improving forecast accuracy.

## Simulating Data

The data used in this paper was generated specifically for the purpose and was **not** gathered at any specific community college. This was done for several reasons:

1. Simulated data are generated with *known* parameters, allowing us to determine how accurately models recover *true* estimates
2. Simulated data allow for generalizability. Data generating processes may differ dramatically across institutions.
3. Simulated data allow us to demonstrate theoretically important concepts by assuring these concepts are the *only* relevant ones in our model.

## Data Generating Process

The data used in this study was generated through two processes. First, the number of new enrollees in a given semester was generated at the aggregate level (i.e. total number of new students). Second, the retention status of enrolled students in a given semester was generated at the individual level (i.e. did student n return at t + 1). Finally, the individual level predictions were aggregated and summed with the number of new enrollees to produce total headcount for a given semester. This process is visualized below. Nodes in yellow are 'outputs' of the generation process which are used in the model fitting section.  

```{r, echo = FALSE}
library('DiagrammeR')

grViz("
digraph data_generation {

  graph[rankdir = TD]

      subgraph cluster_0{
      
      graph[shape = rectangle
      label = 'Individual-level'
      labelloc = t
      fontsize = 25
      rankdir = TD]
      
      node[shape = 'box']
      a[label = 'Students @ t-1']
      b[label = 'Logistic\nregression', shape = 'cylinder']
      c[label = 'Predict probability of return']
      d[label = 'Generate return status']
      e[label = 'Non-returning students']
      f[label = 'Returning Student']
      n[label = 'Random sample', shape = 'cylinder']
      r[label = 'Random sample', shape = 'cylinder']
      t[label = 'Credit load']
      
      {rank = same; b, c}
      {rank = same; e, f}
      {rank = same; r, d}
      {rank = same; n, t}
      
      #edges
      a -> c
      b -> c
      d -> {e, f}
      c -> d
      r -> d
      f -> t
      n -> t
      c -> r[style = dashed]
      }
      
      subgraph cluster_1{
      graph[shape = rectangle
      label = 'Aggregate-level'
      labelloc = t
      fontsize = 25]
      
      node[shape = 'box']
      g[label = 'Number of New Students']
      ab[label = 'Create N individual records']
      h[label = 'OLS', shape = 'cylinder']
      aa[label = 'De-normalize & round', shape = 'cylinder']
      o[label = 'Individual variable values']
      ac[label = 'Random sample', shape = 'cylinder']
      
      {rank = same; g, h, aa}
      {rank = same; o, ac}
      
      #edges
      h -> aa
      aa -> g
      g -> ab
      ab -> o
      ac -> o
      }
      

      node[shape = 'box']
      j[label = 'Join']
      l[label = 'New students']
      q[label = 'Individual records @ t']
      u[label = 'Returning students']
      ad[label = 'Aggregate']
      ae[label = 'Total enrollment @ t']
      
      #edges
      o -> l
      {l, u} -> j
      j -> q
      t -> u
      j -> ad
      ad -> ae
      
      
}")
```

### Returning Enrollees

The retention status of currently enrolled students was generated at the individual level. Three variables are observed for each student: Gender, current semester credit load, and cumulative credits taken. Each student's likelihood of returning in the subsequent semester is a linear function of their gender and curvilinear function of cumulative credits taken such that students are *more* likely to be retained as they approach graduation and *less* likely thereafter. Constant and error terms are also included.

The formula used to generate each student's probability of returning is given below; the coefficients associated with each term as well as the values for the constant and error parameters can be found in the Appendix. As noted above, I chose to generate data using a simple model for the purpose of demonstration; Gender and cumulative credit load are widely cited predictors of retention.

logit(y) = $\beta_{Gender}$ + ${\beta_{Cumulative\ Credits}}^2$ + $\beta_{Cumulative\ Credits}$ + *C* + $\epsilon$

### New Enrollees

The number of new enrollees in a given semester was generated at the aggregate level and is given in heuristic form below. The number of new enrollees in a given semester is a linear function of change in GDP and semester such that higher GDP growth is associated with greater enrollment. Moreover, Fall, Spring, and Summer semester terms each add a constant number of new enrollees in decreasing order. Results from the model were then de-normalized and rounded to generate an integer number of new enrollees. The coefficients associated with each term as well as the values for the constant and error parameters can be found in the Appendix. As with the individual level data, local GDP change and semester effects are widely documented in the literature^[Note that, since GDP data were *not* gathered from actual sources and were instead generated using general sampling process, it could just as easily be interpreted as the percent change in number of graduating high school students. For ease of interpretation and generalizabiilty, I choose to present the variable as change in GDP.].

y = $\beta_{GDP}$ + $\beta_{Spring}$ + $\beta_{Summer}$ + $\beta_{Fall}$ + *C* + $\epsilon$

New Students = ($\sigma_{New\ Students}$ * y) + $\mu_{New\ Students}$

# Method

Modeling choices ought to reflect our underlying knowledge about the data generating process. In this example, we know that the processes that generated our observations vary across sub-populations. Different generative models determine the number of new enrollees and returning students in a given semester. Rather than fitting a single model to the aggregate count of enrollees, which would entail a misalignment between theory and practice and require us to throw away a large amount of information at the individual level, we can estimate a 'stacked' model instead, fitting different models to different sub-populations and aggregating those predictions.

To demonstrate the value in building theory-informed predictive models, I fit and compare several such models. I test autoregressive integrated moving average (ARIMA) and simple linear models at the aggregate level, as these are the most common approaches in the extent literature^[Exponential smoothing approaches are also common early in the enrollment forecasting literature; X, for instance, lists such an approach as one of the most common ways community college practitioners approach modeling. However, this approach sees little use - at least in its simple form - among contemporary practitioners who prefer other methods of time series modeling. For this reason, I elect not to include an exponential smoothing model in my comparison] and compare them to two stacked models. For the first stacked model, I fit a theoretically misspecified model, predicting the number of returning students at the individual level using  non-polynomial terms in a logistic regression and the number of new students using ARIMA. For the second, I fit a correctly specified model with the correct polynomial term.

I compare the results of these models using the Mean Absolute Error (MASE), Mean Absolute Percent Error (MAPE), and Root Mean Squared Error (RMSE), three widely used measures of predictive accuracy. To ensure results are not the product of random chance, I fit each model to 1000 unique datasets and present the mean value for each measure of model accuracy.

# Results

While all tested models predict enrollment with some degree of accuracy, there are large differences between models. Simple linear models without lag terms perform poorest, with MAPE scores near 9%. Simple ARIMA models do not fare much better with MAPE scores hovering near 6%. There are smaller differences between simple linear models with lag terms and stacked models. However, stacked models, both with and without higher order terms, outperform the former, if only by a few tenths of a percent in MAPE.

Though the differences between stacked and linear lagged models are statistically small, their substantive importance should not be dismissed. Such differences can easily mean missing predictions by several hundred students at a community college with a student body in the tens of thousands. Moreover, it should be noted that, while the linear model with a lagged term is more parsimonious than the stacked models, parsimony for its own sake is dubious in circumstances where predictive accuracy is paramount

```{r, echo = FALSE}
library('ggplot2')
library('reshape2')

predictions <- read.csv('predictions.csv')

predictions <- predictions[,-1]

predictions <- predictions[,c(5, grep('prediction', colnames(predictions)))]

predictions$semester <- 1:nrow(predictions)

predictions <- melt(predictions,
                    id.vars = 'semester')

predictions <- predictions[complete.cases(predictions),]

predictions <- predictions[-which(predictions$variable == 'stacked_predictions_1'),]

ggplot(data = predictions) +
  geom_line(aes(x = semester,
                y = value,
                color = variable,
                group = variable)) +
  theme()

```


```{r, echo = FALSE}
diagnostics <- read.csv('diagnostics.csv')

diagnostics <- diagnostics[,-1]

knitr::kable(diagnostics)
```


# Discussion

A skeptical reader may justifiably question the value in the above exercise. After all, of course the better specified model predicted more accurately, we *knew* the true parameters and data generating process before starting. This is, however, exactly the point. Often, practitioners approach enrollment prediction from a point of imposed ignorance. Brute force and automated methods of feature selection are treated as substitutes for theoretically informed modeling choices^[This is not to argue that automated and brute force methods of feature selection are inherently bad. Note that the ARIMA model fit in the stacked version above iterates through all combinations of parameters to find the best fit. Such methods are powerful tools but they are best leveraged when informed by theoretically grounded modeling choices.]. In the community college context, where data availability can be limited and the process that generates enrollment data is idiosyncratic in known ways, discarding such knowledge has practical consequences for the accuracy of our enrollment forecasts and consequently, on resource availability. Practitioners ought to take pains to incorporate theoretical knowledge in predictive forecasts. Employing stacked models, as shown above, can be a simple and effective manner of doing so.

\newpage

# Appendix A: Coefficients, Parameters, and Variable Values

## Coefficients and Model Parameters

```{r, echo = FALSE}
table <- cbind(
c('$\\beta_{GDP}$', '$\\beta_{Spring\ Semester}$', '$\\beta_{Summer\ Semester}$', '$\\beta_{Fall\ Semester}$',
  '$C_{New Students}$', '$\\varepsilon_{New Students}$', '$\\beta_{Gender}$', '$\\beta_{Cumulative\ Credits}$',
  '$C_{Returning Students}$', '$\\varepsilon_{Returning Students}$'),
c(rep('New', 6), rep('Returning', 4)),
c(2, 3, 0, 6, 0, '$\\ N(\\mu = 0, \\sigma = 1)$', 0.1, 0.02, 0.9, '$\\ N(\\mu = 0, \\sigma = 1)$')
)

colnames(table) <- c('Term', 'Model', 'Value')

knitr::kable(table)

```

## Individual Variable Summary Statistics

```{r, echo = FALSE}
table <- cbind(
  c('Gender', 'Credits', 'Cumulative Credits', 'Likelihood of Return', 'Return'),
  c('Binary', 'Interval', 'Interval', 'Ratio', 'Binary'),
  c(0, 1, 1, 0.02, 0),
  c(1, 21, 121, 0.83, 1),
  c(0.5, 9, 135, 0.72, 0.72),
  c('Sample (0:1)', 'Sample Truncated Normal', '$\\sum_{(i,j) = 1}^n n_{i,j}$', 'Linear Function', 'Sample (0:1)'),
  c('P(1) = 0.5', '$\\mu$ = 6, $\\sigma$ = 9', '-', 'See Equation 1', 'P(1) = Likelihood of Return'))

colnames(table) <- c('Variable', 'Level', 'Min', 'Max', 'Mean', 'Generation', 'Parameters')

knitr::kable(table)

```

# Appendix B: Cut Parts

In this paper, I have demonstrated the value in bringing theoretical knowledge to bear on predictive models of enrollment. Community colleges construct such models under unique constraints. Moreover, the financial consequences of inaccurate models is more acutely felt than at other types of institutions of higher education. While the extant literature has sought improvements to predictive accuracy by using 'brute-force' methods - e.g. increasing the number of features in models, implementing more model types, exhaustively searching across model hyper-parameters - I argue that these efforts essentially re-invent the wheel insofar as they 'rediscover' relationships that are already well established. Our theoretical knowledge of the predictors of enrollment and persistence is robust. That knowledge should be reflected in our empirical models.

# Appendix C: Summary Statistics