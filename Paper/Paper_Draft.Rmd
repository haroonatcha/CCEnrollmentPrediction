---
title: 'Predicting Enrollment with Stacked Models: Bringing Together Theory and Empirics'
author: "Haroon Atcha"
date: "6/4/2021"
output:
  html_document: default
  word_document: default
bibliography: Citations.bib
---

```{r, setup, echo = FALSE}
library('ggplot2')
library('reshape2')
library('DiagrammeR')
library('knitr')
library('scales')
```


# Introduction

Enrollment forecasting is one of the most consequential tasks Institutional Research departments undertake. As open enrollment institutions that are uniquely reliant on public funding, community colleges need accurate enrollment forecasts for guaranteeing adequate state-level funding and proper planning. Moreover, community colleges operate under unique data availability constraints and admission processes that make the task difficult.

Despite the importance of this task, the extant literature on enrollment forecasting - especially in the community college context - is slim and relatively disconnected from theoretical advances in the study of the predictors of retention and enrollment. This manifests itself in two main problems:

1. There is little literature that speaks to this problem in the context of community colleges
2. Predictive models rarely reflect theoretical knowledge

First, practitioners operating in open enrollment contexts have little recourse to literature that speaks to their specific circumstances. Much of the literature has been generated by practitioners in selective-enrollment institutions and speaks to issues particular to that context. [@aksenova_enrollment_2006], [@chen_integrated_2008], [@nandeshwar_enrollment_2009], and [@slim_predicting_2018] for instance, exclusively apply their models to the selective enrollment context. Work addressing the problem in the community college context is largely dated. While a highly useful survey of forecasting method used at community colleges, [@bender_community_1981]'s work is now several decades old and understandably is not reflective of modern forecasting tools and approaches. Likewise, [@lawrence_forecasting_1980] is useful as a snapshot of best forecasting practices of the time period, but is similarly dated. [@pennington_community_2002] represents the most recent and comprehensive project addressing this issue. In all, the literature that speaks to predictive modeling of enrollment in the community college context is limited.

Second, predictive models of enrollment do not reflect our theoretical understanding of the processes that are being modeled. Much of the modeling in the extant literature falls into one of two camps: time series approaches at the aggregate level (ARIMA) or linear modeling techniques (OLS). While such approaches produce perfectly acceptable predictions, such models are rarely reflective of the theoretical processes they attempt to capture. [@chen_integrated_2008], for instance, compares predictions from both, ARIMA and linear models though uses the aggregate number of enrollments in a given semester as the target variable despite noting that '[...] the portion of a universityâ€™s student enrollees (freshmen) depends on the number of high school graduates within the state [...] [and] the same pool of eligible enrolled or returning students (one-year lagged OSU enrollment).' The recognition that two distinct sub-populations constitute the population of enrollees is not reflected in the modeling which treats each semester's cohort uniformly^[See also [@pfitzner_shortterm_1987] for an early implementation of time series modeling in the community college context that approaches the problem at the aggregate level]. By contrast, [@trusheim_predictive_2011] does model returning and new enrollees separately, and serves as a good example of using theoretical knowledge as a foundation for in empirical exercises, though the methods used to achieve predictions are fairly unsophisticated.

Here, I argue that, in the interest of predictive accuracy, our statistical models ought to be theoretically informed. To demonstrate this, I present a hypothetical example with generated data, an approach widely used in the study of statistics. I show that bringing theoretical knowledge to bear on predictive problems - even if that knowledge is tentative or incomplete - is a fruitful avenue for improving forecast accuracy.

## Simulating Data

The data used in this paper was generated specifically for the purpose and was **not** gathered at any specific community college. This was done for several reasons:

1. Simulated data are generated with *known* parameters, allowing us to determine how accurately models recover *true* estimates
2. Simulated data allow for generalizability. Enrollment processes may differ dramatically across institutions.
3. Simulated data allow us to demonstrate theoretically important concepts by ensuring these concepts are the *only* relevant ones in our model.

## Data Generating Process

The data used in this study was generated through two processes. First, the number of new enrollees in a given semester was generated at the aggregate level (i.e. total number of new students). Second, the retention status of enrolled students in a given semester was generated at the individual level (i.e. did student n return at t + 1). Finally, the individual level predictions were aggregated and summed with the number of new enrollees to produce total headcount for a given semester. This process is visualized below. Nodes in yellow are 'outputs' of the generative  process which are used in the model fitting section.  

```{r, echo = FALSE}

grViz("
digraph data_generation {

  graph[rankdir = TD]

      subgraph cluster_0{
      
      graph[shape = rectangle
      label = 'Individual-level'
      labelloc = t
      fontsize = 25
      rankdir = TD]
      
      node[shape = 'box']
      a[label = 'Students @ t-1']
      b[label = 'Logistic\nregression', shape = 'cylinder']
      c[label = 'Predict probability of return']
      d[label = 'Generate return status']
      e[label = 'Non-returning students']
      f[label = 'Returning Student']
      n[label = 'Random sample', shape = 'cylinder']
      r[label = 'Random sample', shape = 'cylinder']
      t[label = 'Credit load']
      
      {rank = same; b, c}
      {rank = same; e, f}
      {rank = same; r, d}
      {rank = same; n, t}
      
      #edges
      a -> c
      b -> c
      d -> {e, f}
      c -> d
      r -> d
      f -> t
      n -> t
      c -> r[style = dashed]
      }
      
      subgraph cluster_1{
      graph[shape = rectangle
      label = 'Aggregate-level'
      labelloc = t
      fontsize = 25]
      
      node[shape = 'box']
      g[label = 'Number of New Students']
      ab[label = 'Create N individual records']
      h[label = 'OLS', shape = 'cylinder']
      aa[label = 'De-normalize & round', shape = 'cylinder']
      o[label = 'Individual variable values']
      ac[label = 'Random sample', shape = 'cylinder']
      
      {rank = same; g, h, aa}
      {rank = same; o, ac}
      
      #edges
      h -> aa
      aa -> g
      g -> ab
      ab -> o
      ac -> o
      }
      

      node[shape = 'box']
      j[label = 'Join']
      l[label = 'New students']
      q[label = 'Individual records @ t', style = 'filled', fillcolor = yellow]
      u[label = 'Returning students']
      ad[label = 'Aggregate']
      ae[label = 'Total new enrollment @ t', style = 'filled', fillcolor = yellow]
      
      #edges
      o -> l
      {l, u} -> j
      j -> q
      t -> u
      l -> ad
      ad -> ae
      
      
}")
```

### Returning Enrollees

The retention status of currently enrolled students was generated at the individual level. Three variables are observed for each student: Gender, current semester credit load, and cumulative credits taken. Each student's likelihood of returning in the subsequent semester is a linear function of their gender and curvilinear function of cumulative credits taken such that students are *more* likely to be retained as they approach graduation and *less* likely thereafter. Constant and error terms are also included.

The formula used to generate each student's probability of returning is given below; the coefficients associated with each term as well as the values for the constant and error parameters can be found in the Appendix. As noted above, I chose to generate data using a simple model for the purpose of demonstration; Gender and cumulative credit load are not the only predictors of retention but are widely cited and important variables.

logit(y) = $\beta_{Gender}$ + ${\beta_{Cumulative\ Credits}}^2$ + $\beta_{Cumulative\ Credits}$ + *C* + $\epsilon$

### New Enrollees

The number of new enrollees in a given semester was generated at the aggregate level and is given below. The number of new enrollees in a given semester is a linear function of change in GDP and semester such that higher GDP growth is associated with greater enrollment. Moreover, Fall, Spring, and Summer semester terms each add a constant number of new enrollees in decreasing order. Results from the model were then de-normalized and rounded to generate an integer number of new enrollees. The coefficients associated with each term as well as the values for the constant and error parameters can be found in the Appendix. As with the individual level data, local GDP change and semester effects are widely documented in the literature^[Note that, since GDP data were *not* gathered from actual sources and were instead generated using general sampling process, it could just as easily be interpreted as the percent change in number of graduating high school students. For ease of interpretation and generalizabiilty, I choose to present the variable as change in GDP.].

y = $\beta_{GDP}$ + $\beta_{Spring}$ + $\beta_{Summer}$ + $\beta_{Fall}$ + *C* + $\epsilon$

New Students = ($\sigma_{New\ Students}$ * y) + $\mu_{New\ Students}$

# Method

Modeling choices ought to reflect our knowledge about the underlying data generating process. In this example, we know that the processes that generated our observations vary across sub-populations. Different generative models determine the number of new enrollees and returning students in a given semester. Rather than fitting a single model to the aggregate count of enrollees, which would entail a misalignment between theory and practice and require us to throw away a large amount of information at the individual level, we can estimate a 'stacked' model instead, fitting different models to different sub-populations and aggregating those predictions.

To demonstrate the value in building theory-informed predictive models, I fit and compare several such models. I test autoregressive integrated moving average (ARIMA) and simple linear models at the aggregate level, as these are the most common approaches in the extent literature^[Exponential smoothing approaches are also common early in the enrollment forecasting literature; X, for instance, lists such an approach as one of the most common ways community college practitioners approach modeling. However, this approach sees little use - at least in its simple form - among contemporary practitioners who prefer other methods of time series modeling. For this reason, I elect not to include an exponential smoothing model in my comparison] and compare them to two stacked models. For the first stacked model, I fit a theoretically misspecified model, predicting the number of returning students at the individual level using  non-polynomial terms in a logistic regression and the number of new students using ARIMA. For the second, I fit a correctly specified model with the correct polynomial term.

I compare the results of these models using the Mean Absolute Error (MAE), Mean Absolute Percent Error (MAPE), and Root Mean Squared Error (RMSE), three widely used measures of predictive accuracy. To ensure results are not the product of random chance, I fit each model to 100 uniquely generated data sets and present the mean value and standard error for each measure of model accuracy.

# Results

While all tested models predict enrollment with some degree of accuracy, there are large differences between models. Simple linear models perform significantly worse across all measures of predictive accuracy, with large mean MAE, MAPE, and RMSE scores. Simple ARIMA models do not fare much better though represent an improvement over simple linear models.

```{r, echo = FALSE}

temp <- read.csv('aggregate_metrics.csv')

#calculate standard error for each of my diagnostic
#measure means
st.error <- function(x) sd(x) / sqrt(length(x))

table <- data.frame(
  cbind(
    #mean MAE
    aggregate(temp$MAE,
              by = list(temp$Model),
              FUN = mean),
    #standard error MAE
    aggregate(temp$MAE,
              by = list(temp$Model),
              FUN = st.error)[,2],
    #mean MAPE
    aggregate(temp$MAPE,
              by = list(temp$Model),
              FUN = mean)[,2],
    #standard error MAPE
    aggregate(temp$MAPE,
              by = list(temp$Model),
              FUN = st.error)[,2],
    #mean RMSE
    aggregate(temp$RMSE,
              by = list(temp$Model),
              FUN = mean)[,2],
    #standard error RMSE
    aggregate(temp$RMSE,
              by = list(temp$Model),
              FUN = st.error)[,2]
  ))

colnames(table) <- c('Model', 
                     'MAE Mean', 'MAE St.error',
                     'MAPE Mean', 'MAPE St.error',
                     'RMSE Mean', 'RMSE St.error')

table[,c(2, 3, 6, 7)] <-  round(table[,c(2, 3, 6, 7)], 2)

table[,4] <- percent(table[,4])

table[,5] <- percent(table[,5], 0.001)

kable(table)

```

Comparisons between linear models with lag terms and stacked models illustrates the critical point of this exercise. While linear models with lag terms perform essentially on par with the stacked models in terms of MAE and MAPE, linear models perform significantly worse on RMSE and are more volatile in this regard. In essence, when linear models with lag terms get it wrong, they get it much 'wronger' than stacked models. Additionally, across datasets, the prevalence of highly 'wrong' values varies considerably with linear models that include lag terms; hence its volatility. Only stacked models predict enrollment precisely and accurately.

```{r, echo = FALSE, message = FALSE}
temp <- read.csv('aggregate_metrics.csv')

temp <- temp[,-1]

temp$index <- 1:nrow(temp)

viz <- melt(temp,
            id.vars = c('index', 'Model'))

ggplot(data = viz) +
  geom_histogram(aes(x = value)) +
  facet_grid(rows = vars(viz$Model),
             cols = vars(viz$variable),
             scales = 'free_x')

```

Finally, it should be noted that the correctly specified stacked model does not outperform the parsimonious stacked model though differences are statistically negligible across all measures of predictive accuracy. Neither are volatile in the same way that linear models with lag terms are.

# Discussion

A skeptical reader may justifiably question the value in the above exercise. After all, of course the better specified model predicted more accurately, we *knew* the true parameters and data generating process before starting. This is, however, exactly the point. Often, practitioners approach enrollment prediction from a point of imposed ignorance. Brute force and automated methods of feature selection are treated as substitutes for theoretically informed modeling choices^[This is not to argue that automated and brute force methods of feature selection are inherently bad. Note that the ARIMA model fit in the stacked version above iterates through all combinations of parameters to find the best fit. Such methods are powerful tools but they are best leveraged when informed by theoretically grounded modeling choices.]. In the community college context, where data availability can be limited and the process that generates enrollment data is idiosyncratic in known ways, discarding such knowledge has practical consequences for the accuracy of our enrollment forecasts and consequently, on resource availability. Practitioners ought to take pains to incorporate theoretical knowledge in predictive forecasts. Employing stacked models, as shown above, can be a simple and effective manner of doing so.

\newpage

# Appendix A: Coefficients, Parameters, and Variable Values

## Coefficients and Model Parameters

```{r, echo = FALSE}
table <- cbind(
c('$\\beta_{GDP}$', '$\\beta_{Spring\ Semester}$', '$\\beta_{Summer\ Semester}$', '$\\beta_{Fall\ Semester}$',
  '$C_{New Students}$', '$\\varepsilon_{New Students}$', '$\\beta_{Gender}$', '$\\beta_{Cumulative\ Credits}$',
  '$C_{Returning Students}$', '$\\varepsilon_{Returning Students}$'),
c(rep('New', 6), rep('Returning', 4)),
c(2, 3, 0, 6, 0, '$\\ N(\\mu = 0, \\sigma = 1)$', 0.1, 0.02, 0.9, '$\\ N(\\mu = 0, \\sigma = 1)$')
)

colnames(table) <- c('Term', 'Model', 'Value')

knitr::kable(table)

```

## Individual Variable Summary Statistics

```{r, echo = FALSE}
table <- cbind(
  c('Gender', 'Credits', 'Cumulative Credits', 'Likelihood of Return', 'Return'),
  c('Binary', 'Interval', 'Interval', 'Ratio', 'Binary'),
  c(0, 1, 1, 0.02, 0),
  c(1, 21, 121, 0.83, 1),
  c(0.5, 9, 135, 0.72, 0.72),
  c('Sample (0:1)', 'Sample Truncated Normal', '$\\sum_{(i,j) = 1}^n n_{i,j}$', 'Linear Function', 'Sample (0:1)'),
  c('P(1) = 0.5', '$\\mu$ = 6, $\\sigma$ = 9', '-', 'See Equation 1', 'P(1) = Likelihood of Return'))

colnames(table) <- c('Variable', 'Level', 'Min', 'Max', 'Mean', 'Generation', 'Parameters')

knitr::kable(table)

```

# Works Cited